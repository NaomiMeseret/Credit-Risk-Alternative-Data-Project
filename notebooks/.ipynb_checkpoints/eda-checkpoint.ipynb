{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "## Credit Risk Probability Model - Alternative Data\n",
        "\n",
        "This notebook contains exploratory analysis of the eCommerce transaction data to understand patterns, identify data quality issues, and form hypotheses for feature engineering.\n",
        "\n",
        "**Note**: This notebook is for exploration only, not for production code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Set Up Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Data manipulation\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Visualization\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "# Data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "# Configuration\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2 CSV file(s) in data/raw/\n",
            "  - Xente_Variable_Definitions.csv\n",
            "  - data.csv\n",
            "\n",
            "Loading data from: data.csv\n",
            "Error loading data: name 'pd' is not defined\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 31\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading data from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_file\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(data_file, low_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading data: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 57\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame()\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ],
      "source": [
        "# Define data paths\n",
        "project_root = Path().resolve().parent\n",
        "data_raw_path = project_root / 'data' / 'raw'\n",
        "\n",
        "# Check if data directory exists\n",
        "if not data_raw_path.exists():\n",
        "    print(f\"Data directory not found at {data_raw_path}\")\n",
        "    print(\"Please ensure the data files are placed in the data/raw/ directory\")\n",
        "else:\n",
        "    # List available files\n",
        "    data_files = list(data_raw_path.glob('*.csv'))\n",
        "    print(f\"Found {len(data_files)} CSV file(s) in data/raw/\")\n",
        "    for file in data_files:\n",
        "        print(f\"  - {file.name}\")\n",
        "\n",
        "# Load the main transaction data\n",
        "# Note: Update the filename based on actual data file name\n",
        "try:\n",
        "    # Try common file names\n",
        "    possible_names = ['transactions.csv', 'train.csv', 'data.csv', 'xente.csv']\n",
        "    data_file = None\n",
        "    \n",
        "    for name in possible_names:\n",
        "        file_path = data_raw_path / name\n",
        "        if file_path.exists():\n",
        "            data_file = file_path\n",
        "            break\n",
        "    \n",
        "    if data_file:\n",
        "        print(f\"\\nLoading data from: {data_file.name}\")\n",
        "        df = pd.read_csv(data_file, low_memory=False)\n",
        "        print(f\"Data loaded successfully!\")\n",
        "    else:\n",
        "        print(\"\\nNo data file found. Creating sample structure for demonstration...\")\n",
        "        # Create a sample dataframe structure for demonstration\n",
        "        df = pd.DataFrame({\n",
        "            'TransactionId': [],\n",
        "            'BatchId': [],\n",
        "            'AccountId': [],\n",
        "            'SubscriptionId': [],\n",
        "            'CustomerId': [],\n",
        "            'CurrencyCode': [],\n",
        "            'CountryCode': [],\n",
        "            'ProviderId': [],\n",
        "            'ProductId': [],\n",
        "            'ProductCategory': [],\n",
        "            'ChannelId': [],\n",
        "            'Amount': [],\n",
        "            'Value': [],\n",
        "            'TransactionStartTime': [],\n",
        "            'PricingStrategy': [],\n",
        "            'FraudResult': []\n",
        "        })\n",
        "        print(\"Sample structure created. Please load actual data when available.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading data: {e}\")\n",
        "    df = pd.DataFrame()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Overview of the Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "DATA OVERVIEW\n",
            "================================================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATA OVERVIEW\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame is empty. Please load the data first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Basic information\u001b[39;00m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"DATA OVERVIEW\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if df.empty:\n",
        "    print(\"DataFrame is empty. Please load the data first.\")\n",
        "else:\n",
        "    # Basic information\n",
        "    print(f\"\\nDataset Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "    print(f\"\\nMemory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Display first few rows\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"First 5 Rows:\")\n",
        "    print(\"=\" * 80)\n",
        "    display(df.head())\n",
        "    \n",
        "    # Data types\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Data Types:\")\n",
        "    print(\"=\" * 80)\n",
        "    dtype_info = pd.DataFrame({\n",
        "        'Column': df.dtypes.index,\n",
        "        'Data Type': df.dtypes.values,\n",
        "        'Non-Null Count': df.count().values,\n",
        "        'Null Count': df.isnull().sum().values\n",
        "    })\n",
        "    display(dtype_info)\n",
        "    \n",
        "    # Column names\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Column Names:\")\n",
        "    print(\"=\" * 80)\n",
        "    for i, col in enumerate(df.columns, 1):\n",
        "        print(f\"{i:2d}. {col}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mempty:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSUMMARY STATISTICS - NUMERICAL FEATURES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "if not df.empty:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SUMMARY STATISTICS - NUMERICAL FEATURES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Identify numerical columns\n",
        "    numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    \n",
        "    if numerical_cols:\n",
        "        print(f\"\\nNumerical columns ({len(numerical_cols)}): {', '.join(numerical_cols)}\")\n",
        "        display(df[numerical_cols].describe())\n",
        "        \n",
        "        # Additional statistics\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"ADDITIONAL STATISTICS\")\n",
        "        print(\"=\" * 80)\n",
        "        \n",
        "        additional_stats = pd.DataFrame({\n",
        "            'Column': numerical_cols,\n",
        "            'Skewness': [skew(df[col].dropna()) for col in numerical_cols],\n",
        "            'Kurtosis': [kurtosis(df[col].dropna()) for col in numerical_cols],\n",
        "            'Min': [df[col].min() for col in numerical_cols],\n",
        "            'Max': [df[col].max() for col in numerical_cols],\n",
        "            'Range': [df[col].max() - df[col].min() for col in numerical_cols],\n",
        "            'IQR': [df[col].quantile(0.75) - df[col].quantile(0.25) for col in numerical_cols]\n",
        "        })\n",
        "        display(additional_stats)\n",
        "    else:\n",
        "        print(\"No numerical columns found.\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"SUMMARY STATISTICS - CATEGORICAL FEATURES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Identify categorical columns\n",
        "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    \n",
        "    if categorical_cols:\n",
        "        print(f\"\\nCategorical columns ({len(categorical_cols)}): {', '.join(categorical_cols)}\")\n",
        "        for col in categorical_cols:\n",
        "            print(f\"\\n{col}:\")\n",
        "            print(f\"  Unique values: {df[col].nunique()}\")\n",
        "            print(f\"  Most frequent:\")\n",
        "            display(df[col].value_counts().head(10))\n",
        "    else:\n",
        "        print(\"No categorical columns found.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Distribution of Numerical Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty and 'numerical_cols' in locals() and numerical_cols:\n",
        "    # Create distribution plots for numerical features\n",
        "    n_cols = min(3, len(numerical_cols))\n",
        "    n_rows = (len(numerical_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(numerical_cols):\n",
        "        if idx < len(axes):\n",
        "            ax = axes[idx]\n",
        "            \n",
        "            # Histogram with KDE\n",
        "            df[col].dropna().hist(bins=50, ax=ax, alpha=0.7, edgecolor='black')\n",
        "            ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
        "            ax.set_xlabel(col, fontsize=10)\n",
        "            ax.set_ylabel('Frequency', fontsize=10)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(numerical_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Box plots for outlier detection\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"BOX PLOTS FOR OUTLIER DETECTION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(numerical_cols):\n",
        "        if idx < len(axes):\n",
        "            ax = axes[idx]\n",
        "            \n",
        "            # Box plot\n",
        "            bp = ax.boxplot(df[col].dropna(), vert=True, patch_artist=True)\n",
        "            bp['boxes'][0].set_facecolor('lightblue')\n",
        "            ax.set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
        "            ax.set_ylabel(col, fontsize=10)\n",
        "            ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(numerical_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No numerical columns available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Distribution of Categorical Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty and 'categorical_cols' in locals() and categorical_cols:\n",
        "    # Create bar plots for categorical features\n",
        "    n_cols = min(2, len(categorical_cols))\n",
        "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
        "    \n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 6 * n_rows))\n",
        "    axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "    \n",
        "    for idx, col in enumerate(categorical_cols):\n",
        "        if idx < len(axes):\n",
        "            ax = axes[idx]\n",
        "            \n",
        "            # Get top categories (limit to top 20 for readability)\n",
        "            value_counts = df[col].value_counts().head(20)\n",
        "            \n",
        "            # Bar plot\n",
        "            value_counts.plot(kind='bar', ax=ax, color='steelblue', edgecolor='black')\n",
        "            ax.set_title(f'Distribution of {col} (Top 20)', fontsize=12, fontweight='bold')\n",
        "            ax.set_xlabel(col, fontsize=10)\n",
        "            ax.set_ylabel('Frequency', fontsize=10)\n",
        "            ax.tick_params(axis='x', rotation=45)\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Hide extra subplots\n",
        "    for idx in range(len(categorical_cols), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Pie charts for key categorical features (if reasonable number of categories)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"PIE CHARTS FOR KEY CATEGORICAL FEATURES\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Select features with reasonable number of categories (2-10)\n",
        "    pie_features = [col for col in categorical_cols if 2 <= df[col].nunique() <= 10]\n",
        "    \n",
        "    if pie_features:\n",
        "        n_cols = min(2, len(pie_features))\n",
        "        n_rows = (len(pie_features) + n_cols - 1) // n_cols\n",
        "        \n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 7 * n_rows))\n",
        "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "        \n",
        "        for idx, col in enumerate(pie_features):\n",
        "            if idx < len(axes):\n",
        "                ax = axes[idx]\n",
        "                \n",
        "                value_counts = df[col].value_counts()\n",
        "                ax.pie(value_counts.values, labels=value_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "                ax.set_title(f'{col} Distribution', fontsize=12, fontweight='bold')\n",
        "        \n",
        "        # Hide extra subplots\n",
        "        for idx in range(len(pie_features), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No categorical columns available for visualization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correlation Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty and 'numerical_cols' in locals() and len(numerical_cols) > 1:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CORRELATION MATRIX\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Calculate correlation matrix\n",
        "    corr_matrix = df[numerical_cols].corr()\n",
        "    \n",
        "    # Display correlation matrix\n",
        "    display(corr_matrix)\n",
        "    \n",
        "    # Visualize correlation matrix as heatmap\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))  # Mask upper triangle\n",
        "    sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', \n",
        "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
        "    plt.title('Correlation Matrix of Numerical Features', fontsize=14, fontweight='bold', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Identify high correlations (above threshold)\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"HIGH CORRELATIONS (|r| > 0.7)\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    high_corr_pairs = []\n",
        "    for i in range(len(corr_matrix.columns)):\n",
        "        for j in range(i+1, len(corr_matrix.columns)):\n",
        "            corr_value = corr_matrix.iloc[i, j]\n",
        "            if abs(corr_value) > 0.7:\n",
        "                high_corr_pairs.append({\n",
        "                    'Feature 1': corr_matrix.columns[i],\n",
        "                    'Feature 2': corr_matrix.columns[j],\n",
        "                    'Correlation': corr_value\n",
        "                })\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        high_corr_df = pd.DataFrame(high_corr_pairs)\n",
        "        display(high_corr_df.sort_values('Correlation', key=abs, ascending=False))\n",
        "    else:\n",
        "        print(\"No high correlations found (|r| > 0.7)\")\n",
        "else:\n",
        "    print(\"Insufficient numerical columns for correlation analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Identifying Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"MISSING VALUES ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Calculate missing values\n",
        "    missing_data = pd.DataFrame({\n",
        "        'Column': df.columns,\n",
        "        'Missing Count': df.isnull().sum().values,\n",
        "        'Missing Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
        "    })\n",
        "    missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "    \n",
        "    if len(missing_data) > 0:\n",
        "        print(f\"\\nColumns with missing values: {len(missing_data)}\")\n",
        "        display(missing_data)\n",
        "        \n",
        "        # Visualize missing values\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        missing_data_sorted = missing_data.sort_values('Missing Percentage', ascending=True)\n",
        "        plt.barh(missing_data_sorted['Column'], missing_data_sorted['Missing Percentage'], color='coral')\n",
        "        plt.xlabel('Missing Percentage (%)', fontsize=12)\n",
        "        plt.title('Missing Values by Column', fontsize=14, fontweight='bold')\n",
        "        plt.grid(True, alpha=0.3, axis='x')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"\\n✓ No missing values found in the dataset!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Outlier Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty and 'numerical_cols' in locals() and numerical_cols:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"OUTLIER DETECTION USING IQR METHOD\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    outlier_summary = []\n",
        "    \n",
        "    for col in numerical_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        \n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        \n",
        "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
        "        outlier_count = len(outliers)\n",
        "        outlier_percentage = (outlier_count / len(df)) * 100\n",
        "        \n",
        "        outlier_summary.append({\n",
        "            'Column': col,\n",
        "            'Lower Bound': lower_bound,\n",
        "            'Upper Bound': upper_bound,\n",
        "            'Outlier Count': outlier_count,\n",
        "            'Outlier Percentage': outlier_percentage\n",
        "        })\n",
        "    \n",
        "    outlier_df = pd.DataFrame(outlier_summary)\n",
        "    display(outlier_df.sort_values('Outlier Percentage', ascending=False))\n",
        "    \n",
        "    # Visualize outliers using violin plots\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"VIOLIN PLOTS FOR OUTLIER VISUALIZATION\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Select top 6 numerical features with most outliers for visualization\n",
        "    top_outlier_cols = outlier_df.nlargest(6, 'Outlier Percentage')['Column'].tolist()\n",
        "    \n",
        "    if top_outlier_cols:\n",
        "        n_cols = min(3, len(top_outlier_cols))\n",
        "        n_rows = (len(top_outlier_cols) + n_cols - 1) // n_cols\n",
        "        \n",
        "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5 * n_rows))\n",
        "        axes = axes.flatten() if n_rows > 1 else [axes] if n_cols == 1 else axes\n",
        "        \n",
        "        for idx, col in enumerate(top_outlier_cols):\n",
        "            if idx < len(axes):\n",
        "                ax = axes[idx]\n",
        "                \n",
        "                # Violin plot\n",
        "                parts = ax.violinplot([df[col].dropna()], positions=[0], showmeans=True, showmedians=True)\n",
        "                ax.set_title(f'Violin Plot: {col}', fontsize=12, fontweight='bold')\n",
        "                ax.set_ylabel(col, fontsize=10)\n",
        "                ax.grid(True, alpha=0.3)\n",
        "        \n",
        "        # Hide extra subplots\n",
        "        for idx in range(len(top_outlier_cols), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No numerical columns available for outlier detection.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Time Series Analysis (if TransactionStartTime exists)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not df.empty and 'TransactionStartTime' in df.columns:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"TIME SERIES ANALYSIS\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    try:\n",
        "        # Convert to datetime\n",
        "        df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
        "        \n",
        "        # Extract time features\n",
        "        df['Year'] = df['TransactionStartTime'].dt.year\n",
        "        df['Month'] = df['TransactionStartTime'].dt.month\n",
        "        df['Day'] = df['TransactionStartTime'].dt.day\n",
        "        df['DayOfWeek'] = df['TransactionStartTime'].dt.dayofweek\n",
        "        df['Hour'] = df['TransactionStartTime'].dt.hour\n",
        "        \n",
        "        # Transaction volume over time\n",
        "        daily_transactions = df.groupby(df['TransactionStartTime'].dt.date).size()\n",
        "        \n",
        "        plt.figure(figsize=(15, 6))\n",
        "        plt.plot(daily_transactions.index, daily_transactions.values, linewidth=2)\n",
        "        plt.title('Daily Transaction Volume Over Time', fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Date', fontsize=12)\n",
        "        plt.ylabel('Number of Transactions', fontsize=12)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Transaction amount over time (if Amount column exists)\n",
        "        if 'Amount' in df.columns:\n",
        "            daily_amount = df.groupby(df['TransactionStartTime'].dt.date)['Amount'].sum()\n",
        "            \n",
        "            plt.figure(figsize=(15, 6))\n",
        "            plt.plot(daily_amount.index, daily_amount.values, linewidth=2, color='green')\n",
        "            plt.title('Daily Transaction Amount Over Time', fontsize=14, fontweight='bold')\n",
        "            plt.xlabel('Date', fontsize=12)\n",
        "            plt.ylabel('Total Amount', fontsize=12)\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        \n",
        "        # Day of week patterns\n",
        "        if 'DayOfWeek' in df.columns:\n",
        "            day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "            day_counts = df['DayOfWeek'].value_counts().sort_index()\n",
        "            \n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.bar(range(len(day_names)), [day_counts.get(i, 0) for i in range(7)], color='steelblue')\n",
        "            plt.xticks(range(len(day_names)), day_names, rotation=45)\n",
        "            plt.title('Transaction Volume by Day of Week', fontsize=14, fontweight='bold')\n",
        "            plt.ylabel('Number of Transactions', fontsize=12)\n",
        "            plt.grid(True, alpha=0.3, axis='y')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error in time series analysis: {e}\")\n",
        "else:\n",
        "    print(\"TransactionStartTime column not found or dataframe is empty.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Key Insights and Summary\n",
        "\n",
        "### Top 3-5 Most Important Insights\n",
        "\n",
        "Based on the exploratory data analysis conducted above, here are the key findings:\n",
        "\n",
        "#### Insight 1: Excellent Data Quality with No Missing Values\n",
        "**Finding**: The dataset contains 95,662 transactions across 16 features with **zero missing values**. All columns are complete, which is exceptional for real-world financial data. The data spans 90 days (November 15, 2018 to February 13, 2019) with an average of 1,051 transactions per day.\n",
        "\n",
        "**Implication**: No imputation strategy is needed, which simplifies preprocessing. However, we should still validate data quality through business rules (e.g., negative amounts for credits, positive for debits). The clean dataset allows us to focus on feature engineering and model development without data cleaning overhead.\n",
        "\n",
        "---\n",
        "\n",
        "#### Insight 2: High Correlation Between Amount and Value Features\n",
        "**Finding**: The `Amount` and `Value` features show an extremely high correlation of **0.99**, indicating they are essentially the same variable (Value is the absolute value of Amount). Additionally, both features show severe right-skewness (skewness > 51) with significant outliers - 25.5% of Amount values and 9.4% of Value values are outliers using the IQR method.\n",
        "\n",
        "**Implication**: We should **drop one of these features** (likely keep `Amount` as it contains directional information - positive for debits, negative for credits) to avoid multicollinearity. The extreme skewness and outliers suggest we need robust scaling (e.g., log transformation, robust scaler) or outlier treatment (winsorization, capping) before modeling. The wide range (-1,000,000 to 9,880,000) indicates we may need to handle extreme transactions separately.\n",
        "\n",
        "---\n",
        "\n",
        "#### Insight 3: Highly Imbalanced Fraud and Skewed Transaction Patterns\n",
        "**Finding**: Fraud is extremely rare - only **0.2% of transactions** (193 out of 95,662) are flagged as fraudulent. The transaction distribution is heavily skewed toward two product categories: **financial_services (47.46%)** and **airtime (47.07%)**, with other categories representing less than 2% each. ChannelId_3 dominates with 59.5% of transactions.\n",
        "\n",
        "**Implication**: For fraud-based proxy variables, we'll need to handle severe class imbalance (SMOTE, class weights, or anomaly detection approaches). The concentration in two product categories suggests we should create product category features (e.g., financial_services_ratio, airtime_ratio) at the customer level. The channel distribution indicates most customers use a specific channel, which could be a behavioral signal for credit risk.\n",
        "\n",
        "---\n",
        "\n",
        "#### Insight 4: Significant Outliers in Transaction Amounts Require Careful Handling\n",
        "**Finding**: Using the IQR method, **25.5% of Amount values are outliers** (24,441 transactions), with extreme values ranging from -1,000,000 to 9,880,000. The distribution shows extreme right-skewness (skewness = 51.1) and high kurtosis (3,363), indicating a heavy-tailed distribution with many extreme values.\n",
        "\n",
        "**Implication**: These outliers are likely legitimate high-value transactions rather than errors, so we should **avoid removing them**. Instead, we should use robust statistical methods: (1) Winsorization at the 1st and 99th percentiles, (2) Log transformation (after handling negatives), (3) Robust scaling (using median and IQR), or (4) Create categorical features (e.g., transaction_size_category: small/medium/large/very_large). For RFM features, we should use median-based aggregations which are more robust to outliers than means.\n",
        "\n",
        "---\n",
        "\n",
        "#### Insight 5: Temporal Patterns Show Consistent Daily Activity with Some Variability\n",
        "**Finding**: The data spans exactly **90 days** with daily transaction volumes ranging from 373 to 2,984 transactions (mean: 1,051, median: 926, std: 610). This represents approximately 3 months of transaction history, providing sufficient data for RFM feature engineering and temporal pattern analysis.\n",
        "\n",
        "**Implication**: We should engineer temporal features at the customer level: (1) **Recency**: Days since last transaction, (2) **Frequency**: Transactions per week/month, (3) **Consistency**: Coefficient of variation in daily transaction counts, (4) **Day-of-week patterns**: Which days customers typically transact, (5) **Time-of-day patterns**: Hour of day preferences. The 90-day window is ideal for creating stable customer-level aggregations. We should also consider creating features that capture transaction velocity (transactions per day) and recency decay (weighted by time).\n",
        "\n",
        "---\n",
        "\n",
        "### Next Steps for Feature Engineering\n",
        "\n",
        "Based on these insights, the following feature engineering steps are recommended:\n",
        "\n",
        "1. **RFM Feature Engineering**: Create Recency (days since last transaction), Frequency (transaction count, transactions per week), and Monetary (total amount, median amount, amount variability) features aggregated at the customer level using robust statistics (median, IQR) to handle outliers.\n",
        "\n",
        "2. **Handling Missing Values**: No imputation needed - data is complete. However, validate business rules (e.g., negative amounts should be credits).\n",
        "\n",
        "3. **Outlier Treatment**: Apply winsorization at 1st and 99th percentiles for Amount/Value, or use log transformation after handling negative values. Consider creating transaction size categories (quartiles or business-defined thresholds).\n",
        "\n",
        "4. **Categorical Encoding**: Use target encoding or frequency encoding for high-cardinality categoricals (ProductId, ProviderId). One-hot encode low-cardinality features (ProductCategory, ChannelId). Create customer-level aggregations (e.g., most_used_channel, favorite_product_category_ratio).\n",
        "\n",
        "5. **Temporal Features**: Extract day-of-week, hour-of-day, and create customer-level temporal patterns (preferred_transaction_days, transaction_time_consistency). Calculate transaction velocity and recency-weighted features.\n",
        "\n",
        "6. **Proxy Variable Definition**: Design proxy for credit risk using:\n",
        "   - **Fraud-based signal**: Customers with fraud history or high fraud rate\n",
        "   - **Transaction behavior**: Irregular patterns, sudden changes in spending\n",
        "   - **Payment patterns**: Ratio of credits to debits, negative balance frequency\n",
        "   - **Engagement decay**: Decreasing transaction frequency over time\n",
        "   - **Risk indicators**: High-value transactions, unusual product mix\n",
        "\n",
        "7. **Feature Selection**: Remove `Value` column (redundant with `Amount`). Consider creating interaction features between Amount, ProductCategory, and ChannelId.\n",
        "\n",
        "8. **Data Transformation**: Apply robust scaling or quantile transformation to handle skewness. Consider power transformations (Box-Cox, Yeo-Johnson) for Amount feature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save a summary of key statistics for reference\n",
        "if not df.empty:\n",
        "    summary_stats = {\n",
        "        'total_rows': len(df),\n",
        "        'total_columns': len(df.columns),\n",
        "        'numerical_features': len(numerical_cols) if 'numerical_cols' in locals() else 0,\n",
        "        'categorical_features': len(categorical_cols) if 'categorical_cols' in locals() else 0,\n",
        "        'missing_values_total': df.isnull().sum().sum(),\n",
        "        'duplicate_rows': df.duplicated().sum()\n",
        "    }\n",
        "    \n",
        "    print(\"=\" * 80)\n",
        "    print(\"EDA SUMMARY STATISTICS\")\n",
        "    print(\"=\" * 80)\n",
        "    for key, value in summary_stats.items():\n",
        "        print(f\"{key.replace('_', ' ').title()}: {value:,}\")\n",
        "    \n",
        "    print(\"\\nEDA completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
